# QR Code Detection

This repository provides the full implementation for training and running inference with a YOLOv8 model designed to detect QR codes within images.

The workflow covers dataset preparation and splitting, model training, and inference on unseen images, producing bounding box outputs in both visualized images and a JSON format file.

---

## ðŸ“‚ Project Structure  

```
â”œâ”€â”€ QR_Dataset/
â”‚   â”œâ”€â”€ labels/
â”‚      â”œâ”€â”€ img001.txt
â”‚      â”œâ”€â”€ img002.txt
â”‚      â”œâ”€â”€ ...
â”‚      â””â”€â”€ Annotation text files in YOLO format for each image in the train_images subset of QR_Dataset.
â”‚   â”œâ”€â”€ train_images/        # Training images
â”‚   â”œâ”€â”€ test_images/         # Test images for inference
â”‚   â”œâ”€â”€ split2/  
â”‚         â”œâ”€â”€ labels/              # YOLO format label files (.txt) with split of 80% training and 20% validation. Generated by running train.py
â”‚         â”‚   â”œâ”€â”€ train/
â”‚         â”‚   â””â”€â”€ val/
â”‚         â”œâ”€â”€ images/              #split of 80% training and 20% validation from the train_images in QR_Dataset. Generated by running train.py
â”‚         â”‚   â”œâ”€â”€ train/
â”‚         â”‚   â””â”€â”€ val/
â”‚   â””â”€â”€ data.yaml            # Auto-generated by running train.py
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ model/               # YOLO training outputs (weights, logs)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ annotated_output/        # Annotated inference images
â”‚   â”œâ”€â”€ submission_detection_1.json       # Final detection results
â”‚   â””â”€â”€ submission_decoding_2.json      # Final decoding results
â”‚
â”œâ”€â”€ splitting.py             # Splitting script
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ infer.py                 # Inference script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                # Project documentation

```
---

## âš™ï¸ Environment Setup 

### 1ï¸âƒ£ Install requirements.txt  
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Install dependencies  
```bash
pip install ultralytics
```
## Annotation of training images using CVAT(online annotation)

### 1. Open CVAT
Open a terminal and run:
- Go to https://www.cvat.ai/ and create a free account.
  
### 2. Create a Task
- Navigate to the Tasks section.
- Upload all your dataset images.
- Add a new label (e.g., QR Code).
- Create the task.

### 3. Annotate Images
- Go to the Jobs section and open the created task.
- Use the Rectangle Tool and select the QR Code label.
- Draw bounding boxes around all QR codes in each image.

### 4. Export Annotations in YOLO Format
- Once all images are annotated, return to the Jobs section.
- Click the three dots (â‹®) menu for your task.
- Choose Export Annotations â†’ YOLO TXT format.


> [!NOTE]  
> Only a few images are included in labels and QR_Dataset folder to show the folder structure. During the execution of ``splitting.py`` on Original QR_Dataset you will get the folder structure specified earlier


## ðŸ“‚ Dataset Splitter (first run ```splitting.py``` before ```train.py```)

Use the following script to split your dataset into train and validation sets (80/20) while keeping all YOLO labels in sync with their corresponding images.

```bash
python splitting.py
```

This will:  
- Take images from QR_Dataset/train_images/
- Match each image with its YOLO annotation (.txt) from ```QR_Dataset/labels/```
- Shuffle and split the dataset into 80% training and 20% validation
- Save the new structure inside ```QR_Dataset/split2/```

### Original structure of dataset before running ```splitting.py```

```
â”œâ”€â”€ QR_Dataset/
   â”œâ”€â”€ train_images/        # Training images
   â”œâ”€â”€ test_images/         # Test images for inference
```

### Dataset Structure After Running ```splitting.py```

```
â”œâ”€â”€ QR_Dataset/
   â”œâ”€â”€ train_images/          # Original training images
   â”œâ”€â”€ test_images/           # Test images for inference
   â”œâ”€â”€ labels/                # Original YOLO annotation files (.txt)
   â”œâ”€â”€ split2/                # New split created by script
   â”‚   â”œâ”€â”€ images/
   â”‚   â”‚   â”œâ”€â”€ train/         # 80% training images
   â”‚   â”‚   â””â”€â”€ val/           # 20% validation images
   â”‚   â””â”€â”€ labels/
   â”‚       â”œâ”€â”€ train/         # YOLO labels for training images
   â”‚       â””â”€â”€ val/           # YOLO labels for validation images

```

---



## ðŸš€ Training the model

This part of code uses YOLOv8 to train a  model for detecting QR codes. 

```bash
python train.py
```

What this script does:  
- Loads a pre-trained YOLOv8 model (```yolov8n.pt``` by default)
- Uses your dataset configuration from ```QR_Dataset/data.yml```
- Trains for 50 epochs with augmentations enabled
- Resizes all images to 640Ã—640
- Uses Adam optimizer with learning rate ```0.01```
- Saves results in ```src/models/qr_yolo_model/```
- Best model weights are stored in:
 ```bash
  src/models/qr_yolo_model/weights/best.pt
  ```
---

## ðŸ–¼ï¸ Inference with Trained YOLOv8 Model

This script runs inference on a folder of images using your trained YOLOv8 model, outputs annotated images with bounding boxes, and saves detection results in a JSON file.

```bash
python infer.py
```
> [!NOTE]  
> In the ```infer.py``` script, update the line
> ```bash
> TEST_IMAGES_DIR = "QR_Dataset/test_images"
> ```
> to your own custom folder path. This tells the code where to look for images, and the output (annotated images and JSON) will be generated based on the images inside that folder.

> [!NOTE]  
> In the ```infer.py``` script, make sure this line
> ```bash
> MODEL_FILE = "src/model/qr_yolo_model_aug/weights/best.pt"
> ```
> properly points to weight ```best.pt``` generated by the model in ```src/model```

### ðŸ“‚ Script Overview
- Loads the trained YOLOv8 model from:
  ```bash
  src/models/qr_yolo_model/weights/best.pt
  ```
- Runs predictions on all images inside:
  ```bash
  QR_Dataset/test_images/
  ```
- Saves annotated images (with bounding boxes) to:
  ```bash
  outputs/annotated_image/
  ```
- Exports Detection results in JSON format to:
  ```bash
  outputs/submission_detection_1.json
  ```

- Exports Decoding results in JSON format to:
  ```bash
  outputs/submission_decoding_2.json
  ```

### ðŸ“Š Detection JSON Output Format
Each imageâ€™s results are stored as:
```json
[
  {
    "image_id": "img001",
    "qrs": [
      {"bbox": [x_min, y_min, x_max, y_max]},
      {"bbox": [x_min, y_min, x_max, y_max]}
    ]
  }
]
```

### ðŸ“Š Decoding JSON Output Format
Each imageâ€™s results are stored as:
```json
[
  {
    "image_id": "img001",
    "qrs": [
      {"bbox": [x_min, y_min, x_max, y_max], "value": "5a0SBZ0D","type": "serial"},
      {"bbox": [x_min, y_min, x_max, y_max], "value": "","type": "undetected"}
    ]
  }
]
```

